# Configuration Spark pour HDFS et modifications du notebook
# √Ä copier dans ton notebook Jupyter

# ============================================
# CELLULE 1: Initialisation Spark avec HDFS
# ============================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Cr√©er la session Spark avec configuration HDFS
spark = SparkSession.builder \
    .appName("Analyse Climatique GSOD - HDFS") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .getOrCreate()

# Configurer le niveau de log
spark.sparkContext.setLogLevel("WARN")

print(f"Spark version: {spark.version}")
print(f"‚úì Session Spark initialis√©e avec support HDFS")

# V√©rifier la connexion HDFS
try:
    sc = spark.sparkContext
    hadoop_conf = sc._jsc.hadoopConfiguration()
    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)
    print(f"‚úì Connexion HDFS √©tablie: {fs.getUri()}")
except Exception as e:
    print(f"‚ö†Ô∏è  Erreur de connexion HDFS: {e}")

# ============================================
# CELLULE 2: V√©rification des donn√©es HDFS
# ============================================

import subprocess

# Fonction pour lister les fichiers HDFS
def list_hdfs(path):
    """Liste les fichiers dans un r√©pertoire HDFS"""
    try:
        result = subprocess.run(
            ['docker', 'exec', 'namenode', 'hdfs', 'dfs', '-ls', path],
            capture_output=True,
            text=True
        )
        return result.stdout
    except Exception as e:
        return f"Erreur: {e}"

# V√©rifier les donn√©es disponibles
print("üìÅ Contenu de HDFS /data/gsod:")
print(list_hdfs("/data/gsod"))

# ============================================
# CELLULE 3: D√©finir le sch√©ma (inchang√©)
# ============================================

# D√©finir le sch√©ma pour les donn√©es GSOD
gsod_schema = StructType([
    StructField("STATION", StringType(), True),
    StructField("DATE", StringType(), True),
    StructField("LATITUDE", DoubleType(), True),
    StructField("LONGITUDE", DoubleType(), True),
    StructField("ELEVATION", DoubleType(), True),
    StructField("NAME", StringType(), True),
    StructField("TEMP", DoubleType(), True),
    StructField("TEMP_ATTRIBUTES", IntegerType(), True),
    StructField("DEWP", DoubleType(), True),
    StructField("DEWP_ATTRIBUTES", IntegerType(), True),
    StructField("SLP", DoubleType(), True),
    StructField("SLP_ATTRIBUTES", IntegerType(), True),
    StructField("STP", DoubleType(), True),
    StructField("STP_ATTRIBUTES", IntegerType(), True),
    StructField("VISIB", DoubleType(), True),
    StructField("VISIB_ATTRIBUTES", IntegerType(), True),
    StructField("WDSP", DoubleType(), True),
    StructField("WDSP_ATTRIBUTES", IntegerType(), True),
    StructField("MXSPD", DoubleType(), True),
    StructField("GUST", DoubleType(), True),
    StructField("MAX", DoubleType(), True),
    StructField("MAX_ATTRIBUTES", StringType(), True),
    StructField("MIN", DoubleType(), True),
    StructField("MIN_ATTRIBUTES", StringType(), True),
    StructField("PRCP", DoubleType(), True),
    StructField("PRCP_ATTRIBUTES", StringType(), True),
    StructField("SNDP", DoubleType(), True),
    StructField("FRSHTT", StringType(), True)
])

print("‚úì Sch√©ma d√©fini")

# ============================================
# CELLULE 4: Charger les donn√©es depuis HDFS
# ============================================

# Charger tous les fichiers CSV depuis HDFS
# Utiliser le chemin HDFS complet
hdfs_path = "hdfs://namenode:9000/data/gsod/*/*.csv"

print(f"üì• Chargement des donn√©es depuis: {hdfs_path}")

df = spark.read.csv(
    hdfs_path,
    header=True,
    schema=gsod_schema
)

print(f"‚úì Donn√©es charg√©es")
print(f"Nombre total d'enregistrements: {df.count():,}")
print(f"Nombre de partitions: {df.rdd.getNumPartitions()}")

# Afficher le sch√©ma
df.printSchema()

# ============================================
# CELLULE 5: Aper√ßu des donn√©es
# ============================================

# Afficher quelques enregistrements
print("\nüìä Aper√ßu des donn√©es:")
df.show(5, truncate=False)

# Statistiques descriptives
print("\nüìà Statistiques descriptives:")
df.select("TEMP", "PRCP", "MAX", "MIN").describe().show()

# ============================================
# NOTE: Le reste du notebook reste identique
# ============================================
# Toutes les analyses et requ√™tes SQL fonctionneront de la m√™me mani√®re
# car les donn√©es sont maintenant dans le DataFrame Spark 'df'

print("\n‚úÖ Configuration HDFS termin√©e!")
print("Tu peux maintenant ex√©cuter le reste de ton notebook normalement.")
