{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9182a66",
   "metadata": {},
   "source": [
    "# Apache Spark : Analyse des donn√©es climatiques mondiales\n",
    "\n",
    "## Objectif\n",
    "Analyser les tendances climatiques mondiales √† l'aide de Spark, y compris le nettoyage des donn√©es, l'EDA et l'extraction d'informations.\n",
    "\n",
    "### Jeu de donn√©es\n",
    "**Global Surface Summary of the Day (GSOD)** provenant de NOAA.\n",
    "Source: https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download_section",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. T√©l√©chargement des donn√©es\n",
    "\n",
    "Les donn√©es GSOD sont disponibles par ann√©e. Chaque archive tar.gz contient des fichiers CSV pour toutes les stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_data",
   "metadata": {},
   "source": [
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Cr√©er le r√©pertoire pour les donn√©es\n",
    "data_dir = Path('/home/jovyan/work/data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Liste des ann√©es √† t√©l√©charger (ajustez selon vos besoins)\n",
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "def download_year(year):\n",
    "    \"\"\"T√©l√©charge et extrait les donn√©es pour une ann√©e donn√©e.\"\"\"\n",
    "    url = f\"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/{year}.tar.gz\"\n",
    "    tar_path = data_dir / f\"{year}.tar.gz\"\n",
    "    extract_dir = data_dir / str(year)\n",
    "    \n",
    "    # T√©l√©charger si pas d√©j√† fait\n",
    "    if not tar_path.exists():\n",
    "        print(f\"T√©l√©chargement de {year}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(tar_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"‚úì {year} t√©l√©charg√©\")\n",
    "    \n",
    "    # Extraire\n",
    "    if not extract_dir.exists():\n",
    "        print(f\"Extraction de {year}...\")\n",
    "        with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=extract_dir)\n",
    "        print(f\"‚úì {year} extrait\")\n",
    "    \n",
    "    return extract_dir\n",
    "\n",
    "# T√©l√©charger les donn√©es (d√©commentez pour ex√©cuter)\n",
    "# for year in years:\n",
    "#     download_year(year)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è NOTE: Si le t√©l√©chargement ne fonctionne pas dans le notebook, vous pouvez:\")\n",
    "print(\"1. T√©l√©charger manuellement depuis: https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/\")\n",
    "print(\"2. Placer les fichiers .tar.gz dans /home/jovyan/work/data/\")\n",
    "print(\"3. Extraire avec: tar -xzf ANN√âE.tar.gz -C /home/jovyan/work/data/ANN√âE/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "init_spark",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Initialisation de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_session",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Cr√©er la session Spark en mode local\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analyse Climatique GSOD\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurer le niveau de log\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"‚úì Session Spark initialis√©e\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Chargement et exploration des donn√©es\n",
    "\n",
    "Les fichiers GSOD contiennent les colonnes suivantes:\n",
    "- STATION: Identifiant de la station\n",
    "- DATE: Date de l'observation\n",
    "- LATITUDE, LONGITUDE: Coordonn√©es\n",
    "- ELEVATION: Altitude\n",
    "- NAME: Nom de la station\n",
    "- TEMP: Temp√©rature moyenne (¬∞F)\n",
    "- TEMP_ATTRIBUTES: Attributs temp√©rature\n",
    "- DEWP: Point de ros√©e\n",
    "- SLP: Pression au niveau de la mer\n",
    "- STP: Pression √† la station\n",
    "- VISIB: Visibilit√©\n",
    "- WDSP: Vitesse du vent\n",
    "- MXSPD: Vitesse maximale du vent\n",
    "- GUST: Rafale\n",
    "- MAX: Temp√©rature maximale\n",
    "- MIN: Temp√©rature minimale\n",
    "- PRCP: Pr√©cipitations\n",
    "- SNDP: Profondeur de neige\n",
    "- FRSHTT: Indicateurs m√©t√©o (brouillard, pluie, neige, gr√™le, orage, tornade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_schema",
   "metadata": {},
   "source": [
    "# D√©finir le sch√©ma pour les donn√©es GSOD\n",
    "gsod_schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True),\n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"TEMP_ATTRIBUTES\", IntegerType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"DEWP_ATTRIBUTES\", IntegerType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"SLP_ATTRIBUTES\", IntegerType(), True),\n",
    "    StructField(\"STP\", DoubleType(), True),\n",
    "    StructField(\"STP_ATTRIBUTES\", IntegerType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"VISIB_ATTRIBUTES\", IntegerType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"WDSP_ATTRIBUTES\", IntegerType(), True),\n",
    "    StructField(\"MXSPD\", DoubleType(), True),\n",
    "    StructField(\"GUST\", DoubleType(), True),\n",
    "    StructField(\"MAX\", DoubleType(), True),\n",
    "    StructField(\"MAX_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MIN\", DoubleType(), True),\n",
    "    StructField(\"MIN_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"PRCP\", DoubleType(), True),\n",
    "    StructField(\"PRCP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SNDP\", DoubleType(), True),\n",
    "    StructField(\"FRSHTT\", StringType(), True)\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_csv",
   "metadata": {},
   "source": [
    "# Charger tous les fichiers CSV\n",
    "# Ajustez le chemin selon votre structure de r√©pertoires\n",
    "data_path = \"/home/jovyan/work/data/*/*.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    data_path,\n",
    "    header=True,\n",
    "    schema=gsod_schema\n",
    ")\n",
    "\n",
    "print(f\"Nombre total d'enregistrements: {df.count():,}\")\n",
    "print(f\"Nombre de partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Afficher le sch√©ma\n",
    "df.printSchema()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_sample",
   "metadata": {},
   "source": [
    "# Afficher quelques enregistrements\n",
    "df.show(5, truncate=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_stats",
   "metadata": {},
   "source": [
    "# Statistiques descriptives\n",
    "df.select(\"TEMP\", \"PRCP\", \"MAX\", \"MIN\").describe().show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Nettoyage des donn√©es\n",
    "\n",
    "Les valeurs manquantes ou invalides dans GSOD sont souvent cod√©es comme 9999.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean_data",
   "metadata": {},
   "source": [
    "# Remplacer les valeurs 9999.9 par null et cr√©er l'ann√©e\n",
    "df_clean = df \\\n",
    "    .withColumn(\"TEMP\", when(col(\"TEMP\") == 9999.9, None).otherwise(col(\"TEMP\"))) \\\n",
    "    .withColumn(\"PRCP\", when(col(\"PRCP\") == 99.99, None).otherwise(col(\"PRCP\"))) \\\n",
    "    .withColumn(\"MAX\", when(col(\"MAX\") == 9999.9, None).otherwise(col(\"MAX\"))) \\\n",
    "    .withColumn(\"MIN\", when(col(\"MIN\") == 9999.9, None).otherwise(col(\"MIN\"))) \\\n",
    "    .withColumn(\"year\", year(col(\"DATE\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"DATE\"))) \\\n",
    "    .withColumn(\"TEMP_C\", (col(\"TEMP\") - 32) * 5.0 / 9.0) \\\n",
    "    .withColumn(\"MAX_C\", (col(\"MAX\") - 32) * 5.0 / 9.0) \\\n",
    "    .withColumn(\"MIN_C\", (col(\"MIN\") - 32) * 5.0 / 9.0) \\\n",
    "    .withColumn(\"PRCP_MM\", col(\"PRCP\") * 25.4)\n",
    "\n",
    "# Filtrer les enregistrements avec des temp√©ratures valides\n",
    "df_clean = df_clean.filter(col(\"TEMP\").isNotNull())\n",
    "\n",
    "print(f\"Enregistrements apr√®s nettoyage: {df_clean.count():,}\")\n",
    "\n",
    "# Mettre en cache pour optimiser les requ√™tes suivantes\n",
    "df_clean.cache()\n",
    "df_clean.count()  # Trigger cache\n",
    "\n",
    "print(\"‚úì Donn√©es nettoy√©es et mises en cache\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_missing",
   "metadata": {},
   "source": [
    "# V√©rifier les valeurs manquantes\n",
    "from pyspark.sql.functions import count, when, isnan, col\n",
    "\n",
    "df_clean.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in [\"TEMP_C\", \"PRCP_MM\", \"MAX_C\", \"MIN_C\"]]\n",
    ").show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sql_section",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Analyse avec Spark SQL\n",
    "\n",
    "Cr√©ons une vue temporaire pour utiliser SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_view",
   "metadata": {},
   "source": [
    "# Cr√©er une vue temporaire\n",
    "df_clean.createOrReplaceTempView(\"climate_data\")\n",
    "\n",
    "print(\"‚úì Vue 'climate_data' cr√©√©e\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "questions",
   "metadata": {},
   "source": [
    "### Question 1: Quelle a √©t√© l'ann√©e la plus froide enregistr√©e, et quelle √©tait la temp√©rature moyenne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coldest_year_sql",
   "metadata": {},
   "source": [
    "# Requ√™te SQL pour trouver l'ann√©e la plus froide\n",
    "coldest_year_sql = \"\"\"\n",
    "SELECT \n",
    "    year,\n",
    "    ROUND(AVG(TEMP_C), 2) as avg_temperature_c,\n",
    "    COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "WHERE TEMP_C IS NOT NULL\n",
    "GROUP BY year\n",
    "ORDER BY avg_temperature_c ASC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "coldest_year = spark.sql(coldest_year_sql)\n",
    "coldest_year.show()\n",
    "\n",
    "# Avec DataFrame API\n",
    "coldest_year_df = df_clean \\\n",
    "    .groupBy(\"year\") \\\n",
    "    .agg(\n",
    "        round(avg(\"TEMP_C\"), 2).alias(\"avg_temperature_c\"),\n",
    "        count(\"*\").alias(\"num_records\")\n",
    "    ) \\\n",
    "    .orderBy(\"avg_temperature_c\") \\\n",
    "    .limit(1)\n",
    "\n",
    "print(\"\\nAvec DataFrame API:\")\n",
    "coldest_year_df.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "question2",
   "metadata": {},
   "source": [
    "### Question 2: Quelle station a contribu√© avec le plus grand nombre d'enregistrements ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top_station_sql",
   "metadata": {},
   "source": [
    "# Requ√™te SQL pour trouver la station la plus active\n",
    "top_station_sql = \"\"\"\n",
    "SELECT \n",
    "    STATION,\n",
    "    NAME,\n",
    "    LATITUDE,\n",
    "    LONGITUDE,\n",
    "    COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "GROUP BY STATION, NAME, LATITUDE, LONGITUDE\n",
    "ORDER BY num_records DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_stations = spark.sql(top_station_sql)\n",
    "top_stations.show(10, truncate=False)\n",
    "\n",
    "# Avec DataFrame API\n",
    "top_stations_df = df_clean \\\n",
    "    .groupBy(\"STATION\", \"NAME\", \"LATITUDE\", \"LONGITUDE\") \\\n",
    "    .agg(count(\"*\").alias(\"num_records\")) \\\n",
    "    .orderBy(col(\"num_records\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"\\nAvec DataFrame API:\")\n",
    "top_stations_df.show(10, truncate=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "additional_analysis",
   "metadata": {},
   "source": [
    "### Analyses suppl√©mentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yearly_trends",
   "metadata": {},
   "source": [
    "# Tendances annuelles de temp√©rature\n",
    "yearly_stats = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year,\n",
    "    ROUND(AVG(TEMP_C), 2) as avg_temp_c,\n",
    "    ROUND(AVG(MAX_C), 2) as avg_max_c,\n",
    "    ROUND(AVG(MIN_C), 2) as avg_min_c,\n",
    "    ROUND(AVG(PRCP_MM), 2) as avg_prcp_mm,\n",
    "    COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "WHERE TEMP_C IS NOT NULL\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\")\n",
    "\n",
    "yearly_stats.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly_patterns",
   "metadata": {},
   "source": [
    "# Patterns mensuels\n",
    "monthly_patterns = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    month,\n",
    "    ROUND(AVG(TEMP_C), 2) as avg_temp_c,\n",
    "    ROUND(AVG(PRCP_MM), 2) as avg_prcp_mm,\n",
    "    COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "WHERE TEMP_C IS NOT NULL\n",
    "GROUP BY month\n",
    "ORDER BY month\n",
    "\"\"\")\n",
    "\n",
    "monthly_patterns.show(12)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "viz_section",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualisations\n",
    "\n",
    "### Question 1: Graphe des temp√©ratures moyennes mondiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_temp",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration du style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Convertir en Pandas pour la visualisation\n",
    "yearly_stats_pd = yearly_stats.toPandas()\n",
    "\n",
    "# Graphique des temp√©ratures\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Temp√©rature moyenne par ann√©e\n",
    "ax1.plot(yearly_stats_pd['year'], yearly_stats_pd['avg_temp_c'], \n",
    "         marker='o', linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax1.fill_between(yearly_stats_pd['year'], \n",
    "                  yearly_stats_pd['avg_min_c'], \n",
    "                  yearly_stats_pd['avg_max_c'], \n",
    "                  alpha=0.3, color='#e74c3c')\n",
    "ax1.set_xlabel('Ann√©e', fontsize=12)\n",
    "ax1.set_ylabel('Temp√©rature (¬∞C)', fontsize=12)\n",
    "ax1.set_title('√âvolution des temp√©ratures moyennes mondiales', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Temp√©rature par mois (pattern saisonnier)\n",
    "monthly_patterns_pd = monthly_patterns.toPandas()\n",
    "months = ['Jan', 'F√©v', 'Mar', 'Avr', 'Mai', 'Jun', 'Jul', 'Ao√ª', 'Sep', 'Oct', 'Nov', 'D√©c']\n",
    "ax2.bar(monthly_patterns_pd['month'], monthly_patterns_pd['avg_temp_c'], \n",
    "        color='#3498db', alpha=0.7)\n",
    "ax2.set_xlabel('Mois', fontsize=12)\n",
    "ax2.set_ylabel('Temp√©rature moyenne (¬∞C)', fontsize=12)\n",
    "ax2.set_title('Pattern saisonnier des temp√©ratures', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(1, 13))\n",
    "ax2.set_xticklabels(months)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/jovyan/work/temperature_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique des temp√©ratures sauvegard√©: temperature_trends.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "viz_precip",
   "metadata": {},
   "source": [
    "### Question 2: Graphe des pr√©cipitations mondiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_precipitation",
   "metadata": {},
   "source": [
    "# Graphique des pr√©cipitations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pr√©cipitations par ann√©e\n",
    "ax1.plot(yearly_stats_pd['year'], yearly_stats_pd['avg_prcp_mm'], \n",
    "         marker='s', linewidth=2, markersize=8, color='#2ecc71')\n",
    "ax1.set_xlabel('Ann√©e', fontsize=12)\n",
    "ax1.set_ylabel('Pr√©cipitations moyennes (mm)', fontsize=12)\n",
    "ax1.set_title('√âvolution des pr√©cipitations moyennes mondiales', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pr√©cipitations par mois\n",
    "ax2.bar(monthly_patterns_pd['month'], monthly_patterns_pd['avg_prcp_mm'], \n",
    "        color='#1abc9c', alpha=0.7)\n",
    "ax2.set_xlabel('Mois', fontsize=12)\n",
    "ax2.set_ylabel('Pr√©cipitations moyennes (mm)', fontsize=12)\n",
    "ax2.set_title('Pattern saisonnier des pr√©cipitations', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(1, 13))\n",
    "ax2.set_xticklabels(months)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/jovyan/work/precipitation_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique des pr√©cipitations sauvegard√©: precipitation_trends.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "warming_analysis",
   "metadata": {},
   "source": [
    "### Question 3: Analyses suppl√©mentaires - R√©chauffement climatique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming_viz",
   "metadata": {},
   "source": [
    "# Calculer la tendance du r√©chauffement\n",
    "from scipy import stats\n",
    "\n",
    "# R√©gression lin√©aire pour la tendance\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "    yearly_stats_pd['year'], \n",
    "    yearly_stats_pd['avg_temp_c']\n",
    ")\n",
    "\n",
    "# Cr√©er la ligne de tendance\n",
    "trend_line = slope * yearly_stats_pd['year'] + intercept\n",
    "\n",
    "# Visualisation de la tendance\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(yearly_stats_pd['year'], yearly_stats_pd['avg_temp_c'], \n",
    "         marker='o', linewidth=2, markersize=8, label='Temp√©rature observ√©e', color='#e74c3c')\n",
    "plt.plot(yearly_stats_pd['year'], trend_line, \n",
    "         '--', linewidth=2, label=f'Tendance ({slope:.4f}¬∞C/an)', color='#c0392b')\n",
    "plt.xlabel('Ann√©e', fontsize=12)\n",
    "plt.ylabel('Temp√©rature (¬∞C)', fontsize=12)\n",
    "plt.title('Tendance du r√©chauffement climatique', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Anomalie par rapport √† la moyenne\n",
    "plt.subplot(2, 1, 2)\n",
    "mean_temp = yearly_stats_pd['avg_temp_c'].mean()\n",
    "anomaly = yearly_stats_pd['avg_temp_c'] - mean_temp\n",
    "colors = ['#e74c3c' if x > 0 else '#3498db' for x in anomaly]\n",
    "plt.bar(yearly_stats_pd['year'], anomaly, color=colors, alpha=0.7)\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.xlabel('Ann√©e', fontsize=12)\n",
    "plt.ylabel('Anomalie de temp√©rature (¬∞C)', fontsize=12)\n",
    "plt.title(f'Anomalie de temp√©rature (R√©f√©rence: {mean_temp:.2f}¬∞C)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/jovyan/work/warming_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistiques de tendance:\")\n",
    "print(f\"   Taux de r√©chauffement: {slope:.4f}¬∞C par an\")\n",
    "print(f\"   R¬≤: {r_value**2:.4f}\")\n",
    "print(f\"   P-value: {p_value:.4e}\")\n",
    "print(f\"   Augmentation totale sur la p√©riode: {slope * len(yearly_stats_pd):.2f}¬∞C\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"   ‚úì La tendance est statistiquement significative (p < 0.05)\")\n",
    "else:\n",
    "    print(\"   ‚ö† La tendance n'est pas statistiquement significative\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "country_analysis",
   "metadata": {},
   "source": [
    "### Question 4: Augmentation de temp√©rature par pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_country",
   "metadata": {},
   "source": [
    "# Extraire le pays du nom de la station (simplifi√© - bas√© sur les deux derni√®res lettres)\n",
    "# Note: Une approche plus robuste utiliserait un dataset de g√©olocalisation\n",
    "\n",
    "# Calculer la temp√©rature moyenne globale sur toute la p√©riode\n",
    "global_avg_temp = df_clean.select(avg(\"TEMP_C\")).collect()[0][0]\n",
    "\n",
    "print(f\"Temp√©rature moyenne mondiale: {global_avg_temp:.2f}¬∞C\")\n",
    "\n",
    "# Analyser par r√©gion g√©ographique (bas√© sur la latitude)\n",
    "df_regions = df_clean.withColumn(\n",
    "    \"region\",\n",
    "    when(col(\"LATITUDE\") >= 60, \"Arctique/Subarctique\")\n",
    "    .when((col(\"LATITUDE\") >= 30) & (col(\"LATITUDE\") < 60), \"Zone temp√©r√©e Nord\")\n",
    "    .when((col(\"LATITUDE\") >= 0) & (col(\"LATITUDE\") < 30), \"Zone tropicale Nord\")\n",
    "    .when((col(\"LATITUDE\") >= -30) & (col(\"LATITUDE\") < 0), \"Zone tropicale Sud\")\n",
    "    .when((col(\"LATITUDE\") >= -60) & (col(\"LATITUDE\") < -30), \"Zone temp√©r√©e Sud\")\n",
    "    .otherwise(\"Antarctique/Subantarctique\")\n",
    ")\n",
    "\n",
    "# Calculer l'augmentation par r√©gion\n",
    "regional_temps = df_regions.groupBy(\"region\", \"year\") \\\n",
    "    .agg(avg(\"TEMP_C\").alias(\"avg_temp\"))\n",
    "\n",
    "# Calculer la moyenne par r√©gion\n",
    "regional_avg = df_regions.groupBy(\"region\") \\\n",
    "    .agg(\n",
    "        avg(\"TEMP_C\").alias(\"avg_temp_c\"),\n",
    "        count(\"*\").alias(\"num_records\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"pct_difference_from_global\",\n",
    "        ((col(\"avg_temp_c\") - global_avg_temp) / global_avg_temp * 100)\n",
    "    ) \\\n",
    "    .orderBy(col(\"pct_difference_from_global\").desc())\n",
    "\n",
    "regional_avg_pd = regional_avg.toPandas()\n",
    "regional_avg_pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_regional",
   "metadata": {},
   "source": [
    "# Visualisation par r√©gion\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Temp√©rature moyenne par r√©gion\n",
    "colors_regional = plt.cm.RdYlBu_r(np.linspace(0, 1, len(regional_avg_pd)))\n",
    "ax1.barh(regional_avg_pd['region'], regional_avg_pd['avg_temp_c'], color=colors_regional)\n",
    "ax1.axvline(x=global_avg_temp, color='red', linestyle='--', linewidth=2, label=f'Moyenne mondiale: {global_avg_temp:.2f}¬∞C')\n",
    "ax1.set_xlabel('Temp√©rature moyenne (¬∞C)', fontsize=12)\n",
    "ax1.set_title('Temp√©rature moyenne par r√©gion', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Pourcentage de diff√©rence par rapport √† la moyenne mondiale\n",
    "colors_pct = ['#e74c3c' if x > 0 else '#3498db' for x in regional_avg_pd['pct_difference_from_global']]\n",
    "ax2.barh(regional_avg_pd['region'], regional_avg_pd['pct_difference_from_global'], color=colors_pct, alpha=0.7)\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax2.set_xlabel('Diff√©rence par rapport √† la moyenne mondiale (%)', fontsize=12)\n",
    "ax2.set_title('√âcart de temp√©rature par r√©gion', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, (idx, row) in enumerate(regional_avg_pd.iterrows()):\n",
    "    ax2.text(row['pct_difference_from_global'], i, f\" {row['pct_difference_from_global']:.1f}%\", \n",
    "             va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/jovyan/work/regional_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique r√©gional sauvegard√©: regional_analysis.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Conclusions et analyses suppl√©mentaires recommand√©es\n",
    "\n",
    "### Analyses recommand√©es:\n",
    "\n",
    "1. **Analyse temporelle avanc√©e:**\n",
    "   - D√©composition saisonni√®re (trend, saisonnalit√©, r√©sidus)\n",
    "   - D√©tection de points de rupture dans les s√©ries temporelles\n",
    "   - Pr√©visions avec mod√®les ARIMA/Prophet\n",
    "\n",
    "2. **Analyse spatiale:**\n",
    "   - Cartographie des hotspots de r√©chauffement\n",
    "   - Analyse de corr√©lation spatiale (Moran's I)\n",
    "   - Identification des zones √† risque √©lev√©\n",
    "\n",
    "3. **Analyse des √©v√©nements extr√™mes:**\n",
    "   - Fr√©quence et intensit√© des vagues de chaleur\n",
    "   - √âv√©nements de pr√©cipitations extr√™mes\n",
    "   - P√©riodes de s√©cheresse prolong√©e\n",
    "\n",
    "4. **Corr√©lations avec d'autres facteurs:**\n",
    "   - Altitude et temp√©rature\n",
    "   - Proximit√© oc√©anique et variabilit√© climatique\n",
    "   - Latitude et amplitude thermique\n",
    "\n",
    "5. **Machine Learning:**\n",
    "   - Pr√©diction des temp√©ratures futures\n",
    "   - Classification des r√©gimes climatiques\n",
    "   - D√©tection d'anomalies\n",
    "\n",
    "### Isolation des effets du r√©chauffement climatique:\n",
    "\n",
    "Pour isoler les effets du r√©chauffement:\n",
    "- Comparer les donn√©es r√©centes (2020-2023) avec des p√©riodes de r√©f√©rence (1980-2000)\n",
    "- Analyser les tendances √† long terme (plusieurs d√©cennies)\n",
    "- Utiliser des mod√®les de r√©gression pour contr√¥ler les variations naturelles\n",
    "- Comparer avec les mod√®les climatiques du GIEC\n",
    "- Analyser les indicateurs secondaires (fonte des glaces, √©l√©vation du niveau de la mer, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme_events",
   "metadata": {},
   "source": [
    "# Analyse des √©v√©nements extr√™mes\n",
    "extreme_events = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    year,\n",
    "    COUNT(CASE WHEN MAX_C > 40 THEN 1 END) as days_above_40c,\n",
    "    COUNT(CASE WHEN MIN_C < -20 THEN 1 END) as days_below_minus20c,\n",
    "    COUNT(CASE WHEN PRCP_MM > 100 THEN 1 END) as heavy_rainfall_days,\n",
    "    MAX(MAX_C) as max_temp_recorded,\n",
    "    MIN(MIN_C) as min_temp_recorded\n",
    "FROM climate_data\n",
    "WHERE MAX_C IS NOT NULL AND MIN_C IS NOT NULL\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\")\n",
    "\n",
    "extreme_events.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_extremes",
   "metadata": {},
   "source": [
    "# Visualisation des √©v√©nements extr√™mes\n",
    "extreme_pd = extreme_events.toPandas()\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Jours > 40¬∞C\n",
    "ax1.bar(extreme_pd['year'], extreme_pd['days_above_40c'], color='#e74c3c', alpha=0.7)\n",
    "ax1.set_title('Jours avec temp√©ratures > 40¬∞C', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Nombre de jours', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Jours < -20¬∞C\n",
    "ax2.bar(extreme_pd['year'], extreme_pd['days_below_minus20c'], color='#3498db', alpha=0.7)\n",
    "ax2.set_title('Jours avec temp√©ratures < -20¬∞C', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Nombre de jours', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Jours de fortes pr√©cipitations\n",
    "ax3.bar(extreme_pd['year'], extreme_pd['heavy_rainfall_days'], color='#2ecc71', alpha=0.7)\n",
    "ax3.set_title('Jours avec pr√©cipitations > 100mm', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Nombre de jours', fontsize=11)\n",
    "ax3.set_xlabel('Ann√©e', fontsize=11)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Temp√©ratures record\n",
    "ax4.plot(extreme_pd['year'], extreme_pd['max_temp_recorded'], \n",
    "         marker='o', label='Max record', color='#e74c3c', linewidth=2)\n",
    "ax4.plot(extreme_pd['year'], extreme_pd['min_temp_recorded'], \n",
    "         marker='s', label='Min record', color='#3498db', linewidth=2)\n",
    "ax4.set_title('Temp√©ratures record par ann√©e', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Temp√©rature (¬∞C)', fontsize=11)\n",
    "ax4.set_xlabel('Ann√©e', fontsize=11)\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/jovyan/work/extreme_events.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique des √©v√©nements extr√™mes sauvegard√©: extreme_events.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. R√©sum√© des requ√™tes SQL utilis√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql_summary",
   "metadata": {},
   "source": [
    "print(\"\"\"\n",
    "=== REQU√äTES SQL PRINCIPALES ===\n",
    "\n",
    "1. ANN√âE LA PLUS FROIDE:\n",
    "SELECT year, ROUND(AVG(TEMP_C), 2) as avg_temperature_c, COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "WHERE TEMP_C IS NOT NULL\n",
    "GROUP BY year\n",
    "ORDER BY avg_temperature_c ASC\n",
    "LIMIT 1\n",
    "\n",
    "2. STATION LA PLUS ACTIVE:\n",
    "SELECT STATION, NAME, LATITUDE, LONGITUDE, COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "GROUP BY STATION, NAME, LATITUDE, LONGITUDE\n",
    "ORDER BY num_records DESC\n",
    "LIMIT 10\n",
    "\n",
    "3. TENDANCES ANNUELLES:\n",
    "SELECT year, \n",
    "       ROUND(AVG(TEMP_C), 2) as avg_temp_c,\n",
    "       ROUND(AVG(MAX_C), 2) as avg_max_c,\n",
    "       ROUND(AVG(MIN_C), 2) as avg_min_c,\n",
    "       ROUND(AVG(PRCP_MM), 2) as avg_prcp_mm,\n",
    "       COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "WHERE TEMP_C IS NOT NULL\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\n",
    "4. PATTERNS MENSUELS:\n",
    "SELECT month,\n",
    "       ROUND(AVG(TEMP_C), 2) as avg_temp_c,\n",
    "       ROUND(AVG(PRCP_MM), 2) as avg_prcp_mm,\n",
    "       COUNT(*) as num_records\n",
    "FROM climate_data\n",
    "WHERE TEMP_C IS NOT NULL\n",
    "GROUP BY month\n",
    "ORDER BY month\n",
    "\n",
    "5. √âV√âNEMENTS EXTR√äMES:\n",
    "SELECT year,\n",
    "       COUNT(CASE WHEN MAX_C > 40 THEN 1 END) as days_above_40c,\n",
    "       COUNT(CASE WHEN MIN_C < -20 THEN 1 END) as days_below_minus20c,\n",
    "       COUNT(CASE WHEN PRCP_MM > 100 THEN 1 END) as heavy_rainfall_days,\n",
    "       MAX(MAX_C) as max_temp_recorded,\n",
    "       MIN(MIN_C) as min_temp_recorded\n",
    "FROM climate_data\n",
    "WHERE MAX_C IS NOT NULL AND MIN_C IS NOT NULL\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Nettoyage et fermeture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "source": [
    "# Exporter les r√©sultats principaux\n",
    "yearly_stats.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    \"/home/jovyan/work/results/yearly_statistics\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "regional_avg.coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    \"/home/jovyan/work/results/regional_analysis\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"‚úì R√©sultats export√©s dans /home/jovyan/work/results/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stop_spark",
   "metadata": {},
   "source": [
    "# Fermer la session Spark\n",
    "# spark.stop()\n",
    "# print(\"‚úì Session Spark ferm√©e\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "final_notes",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes finales\n",
    "\n",
    "### Performance et optimisation:\n",
    "- Les donn√©es ont √©t√© mises en cache pour acc√©l√©rer les requ√™tes r√©p√©t√©es\n",
    "- Utilisation de `coalesce()` pour r√©duire le nombre de partitions lors des exports\n",
    "- Les conversions vers Pandas sont limit√©es aux r√©sultats agr√©g√©s pour la visualisation\n",
    "\n",
    "### Points d'am√©lioration possibles:\n",
    "1. Int√©gration avec HDFS pour le stockage distribu√©\n",
    "2. Utilisation de Spark Structured Streaming pour l'analyse en temps r√©el\n",
    "3. Int√©gration avec MLlib pour des pr√©dictions avanc√©es\n",
    "4. Dashboard interactif avec Plotly Dash ou Streamlit\n",
    "5. Pipeline ETL automatis√© avec Airflow\n",
    "\n",
    "### Ressources suppl√©mentaires:\n",
    "- Documentation GSOD: https://www.ncei.noaa.gov/data/global-summary-of-the-day/doc/\n",
    "- Spark SQL Guide: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- Climate Data Analysis: https://www.ipcc.ch/\n",
    "\n",
    "---\n",
    "**Auteur:** Analyse climatique avec Apache Spark  \n",
    "**Date:** 2024  \n",
    "**Version:** 1.0\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
